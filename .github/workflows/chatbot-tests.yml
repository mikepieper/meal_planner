name: Chatbot Automated Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run critical path tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      run: |
        # Run most critical test scenarios
        python run_tests.py run --scenarios \
          busy_professional_weekly \
          parent_allergies_quick \
          diabetic_senior_daily
    
    - name: Generate test summary
      if: always()
      run: |
        python -c "
        from src.testing.test_utilities import TestAnalyzer
        import json
        
        analyzer = TestAnalyzer()
        stats = analyzer.get_summary_stats()
        
        # Save summary for later steps
        with open('test_summary.json', 'w') as f:
            json.dump(stats, f, indent=2)
        
        # Print summary
        print('=== Test Summary ===')
        print(f\"Total tests: {stats['total_tests']}\")
        print(f\"Average score: {stats['average_scores']['overall']}\")
        
        # Check if we meet quality gates
        if stats['average_scores']['overall'] < 0.7:
            print('‚ùå Quality gate failed: Score below 0.7')
            exit(1)
        else:
            print('‚úÖ Quality gate passed')
        "
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ github.run_id }}
        path: |
          test_results/
          test_summary.json
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let summary;
          try {
            summary = JSON.parse(fs.readFileSync('test_summary.json', 'utf8'));
          } catch (e) {
            console.log('Could not read test summary');
            return;
          }
          
          const passed = summary.recommendations?.pass || 0;
          const total = summary.total_tests || 0;
          const score = summary.average_scores?.overall || 0;
          
          const emoji = score >= 0.8 ? '‚úÖ' : score >= 0.6 ? '‚ö†Ô∏è' : '‚ùå';
          
          const comment = `## ${emoji} Chatbot Test Results
          
          **Overall Score**: ${score.toFixed(2)}/1.00
          **Tests Passed**: ${passed}/${total}
          
          <details>
          <summary>üìä Detailed Scores</summary>
          
          | Metric | Score |
          |--------|-------|
          | Goal Achievement | ${summary.average_scores.goal_achievement.toFixed(2)} |
          | Efficiency | ${summary.average_scores.efficiency.toFixed(2)} |
          | Clarity | ${summary.average_scores.clarity.toFixed(2)} |
          | User Satisfaction | ${summary.average_scores.satisfaction.toFixed(2)} |
          
          </details>
          
          ${summary.top_pain_points?.length > 0 ? `
          <details>
          <summary>üîç Top Issues Found</summary>
          
          ${summary.top_pain_points.slice(0, 3).map(([issue, count]) => 
            `- ${issue} (${count} occurrences)`
          ).join('\n')}
          
          </details>
          ` : ''}
          
          ${score < 0.7 ? '‚ö†Ô∏è **Action Required**: Score is below quality threshold of 0.7' : ''}
          
          [View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  nightly-full-test:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run all tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
      run: python run_tests.py run --all
    
    - name: Generate trend report
      run: |
        python -c "
        from src.testing.test_utilities import TestAnalyzer
        analyzer = TestAnalyzer()
        analyzer.create_trend_report()
        "
    
    - name: Upload reports
      uses: actions/upload-artifact@v3
      with:
        name: nightly-test-reports-${{ github.run_id }}
        path: |
          test_results/
          trend_analysis.html
          test_summary.csv
    
    - name: Notify on failure
      if: failure()
      uses: slackapi/slack-github-action@v1.24.0
      with:
        payload: |
          {
            "text": "‚ùå Nightly chatbot tests failed!",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "‚ùå *Nightly Chatbot Tests Failed*\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Results>"
                }
              }
            ]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  regression-check:
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download current results
      uses: actions/download-artifact@v3
      with:
        name: test-results-${{ github.run_id }}
        path: current_results/
    
    - name: Download baseline results
      uses: dawidd6/action-download-artifact@v2
      with:
        workflow: chatbot-tests.yml
        branch: main
        name: baseline-results
        path: baseline_results/
      continue-on-error: true
    
    - name: Run regression analysis
      if: success()
      run: |
        python -c "
        from src.testing.test_utilities import run_regression_tests
        import json
        import os
        
        if os.path.exists('baseline_results'):
            report = run_regression_tests('baseline_results', 'current_results')
            
            with open('regression_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            if report['summary']['regressed'] > 0:
                print(f\"‚ö†Ô∏è {report['summary']['regressed']} scenarios regressed!\")
                # Don't fail the build, just warn
            else:
                print('‚úÖ No regressions detected')
        else:
            print('No baseline found, skipping regression check')
        "
    
    - name: Update baseline
      if: success()
      uses: actions/upload-artifact@v3
      with:
        name: baseline-results
        path: current_results/
        retention-days: 30 